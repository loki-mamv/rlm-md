<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>rlm.md -- Recursive Language Models</title>
    <meta name="description" content="Recursive Language Models (RLMs) let LLMs process arbitrarily long inputs by treating the prompt as an external environment and recursively calling themselves over snippets. From the MIT OASYS lab.">
    <meta name="keywords" content="recursive language models, RLM, long context, inference-time scaling, RLM-Qwen3-8B, GPT-5, context window, REPL, decompose recurse aggregate">
    <link rel="canonical" href="https://rlm.md/">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://rlm.md/">
    <meta property="og:title" content="rlm.md -- Recursive Language Models">
    <meta property="og:description" content="How RLMs let language models process inputs 100x beyond their context windows by recursively calling themselves over snippets.">
    <meta property="og:site_name" content="rlm.md">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="rlm.md -- Recursive Language Models">
    <meta name="twitter:description" content="The inference paradigm that makes context windows irrelevant.">
    <script type="application/ld+json">{"@context":"https://schema.org","@type":"WebSite","url":"https://rlm.md","name":"rlm.md","description":"Recursive Language Models: how LLMs process arbitrarily long inputs by treating prompts as external environments and recursively self-calling over snippets."}</script>
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'%3E%3Ctext x='16' y='24' font-family='monospace' font-weight='800' font-size='14' fill='%23c9a84c' text-anchor='middle'%3Erlm%3C/text%3E%3C/svg%3E">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav class="site-nav">
    <a href="index.html" class="site-logo">rlm<span class="dot">.md</span></a>
    <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Menu">///</button>
    <ul class="nav-links">
        <li><a href="./" class="active">Home</a></li>
        <li><a href="fundamentals.html">Fundamentals</a></li>
        <li><a href="techniques.html">Techniques</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li><a href="resources.html">Resources</a></li>
    </ul>
</nav>

<div class="page-body">
<div class="container">

    <section class="hero fade-in">
        <span class="hero-label">Recursive Language Models</span>
        <h1>Stop cramming. <span class="accent">Start recursing.</span></h1>
        <p class="hero-sub">Context windows are a leash. Every LLM has one, and every LLM eventually chokes on it. Recursive Language Models cut the leash entirely -- they let a model process inputs two orders of magnitude beyond its context window by treating the prompt as an external environment and recursively calling itself over snippets.</p>
        <p class="hero-sub" style="color: var(--text-dim); font-size: 0.9rem;">RLMs come from the <a href="https://arxiv.org/abs/2512.24601" target="_blank">MIT OASYS lab</a> (Zhang, Kraska, Khattab). An 8B model post-trained with this approach outperforms vanilla GPT-5 on long-context tasks. Start with <a href="fundamentals.html">how it works</a>, or see <a href="research.html">the benchmark results</a>.</p>
    </section>

    <hr class="divider">

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The problem</span>
                <h2>Context windows are a lie</h2>
            </div>
            <div>
                <p>GPT-5 advertises a 272K token context window. Sounds generous. But feed it a task that requires dense reasoning over all 272K tokens -- not just finding a needle, but actually processing every line -- and performance falls off a cliff. This is called <strong>context rot</strong>, and every model suffers from it.</p>
                <p>The industry response has been to make windows bigger. 1M tokens. 10M tokens. But bigger windows don't solve the fundamental issue: Transformers degrade on long, information-dense inputs regardless of what fits technically.</p>
                <p>RLMs take a different approach. Instead of forcing the entire prompt through the neural network at once, they let the model <strong>programmatically examine, decompose, and recursively call itself</strong> over pieces of the input. The prompt lives in a REPL environment as a variable. The model writes code to slice it, process the slices, and aggregate results.</p>
                <p>The result: effective processing of 10M+ token inputs. Not with summarization hacks or retrieval tricks. With actual dense semantic work across the entire input.</p>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <span class="split-label">Start here</span>
        <h2>The complete picture, in five parts.</h2>
        <div class="card-grid stagger" style="margin-top: 2rem;">
            <a href="fundamentals.html" class="card" style="--i:0; text-decoration:none; border-bottom:none;">
                <div class="card-number">01</div>
                <h3>Fundamentals</h3>
                <p class="card p">The decompose-recurse-aggregate pattern. How prompts become environment variables. Why symbolic recursion changes everything.</p>
            </a>
            <a href="techniques.html" class="card" style="--i:1; text-decoration:none; border-bottom:none;">
                <div class="card-number">02</div>
                <h3>Techniques</h3>
                <p>Programmatic examination, decomposition strategies, REPL environments, sub-call patterns. How RLM-Qwen3-8B was post-trained. RLMs vs RAG vs sliding window.</p>
            </a>
            <a href="research.html" class="card" style="--i:2; text-decoration:none; border-bottom:none;">
                <div class="card-number">03</div>
                <h3>Research</h3>
                <p>The arXiv paper dissected. Benchmark results across S-NIAH, OOLONG, BrowseComp-Plus, and CodeQA. RLM-Qwen3-8B vs GPT-5, head to head.</p>
            </a>
        </div>
        <div class="card-grid stagger" style="margin-top: 1.5rem; grid-template-columns: 1fr 1fr;">
            <a href="applications.html" class="card" style="--i:3; text-decoration:none; border-bottom:none;">
                <div class="card-number">04</div>
                <h3>Applications</h3>
                <p>Where RLMs change the game: book-length analysis, legal document processing, codebase understanding, deep research over massive corpora.</p>
            </a>
            <a href="resources.html" class="card" style="--i:4; text-decoration:none; border-bottom:none;">
                <div class="card-number">05</div>
                <h3>Resources</h3>
                <p>The paper, the GitHub repo, the pip package, sandbox options, Google ADK integration, and related work on long-context processing.</p>
            </a>
        </div>
    </section>

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The headline numbers</span>
                <h2>This isn't theoretical</h2>
            </div>
            <div>
                <p>RLM-Qwen3-8B -- an 8-billion parameter model post-trained on just 1,000 samples -- outperforms the base Qwen3-8B by <strong>28.3% on average</strong> across four diverse long-context benchmarks. It approaches the quality of vanilla GPT-5 on three of them.</p>
                <p>At the frontier scale, RLM(GPT-5) maintains strong performance on inputs up to 2^18 tokens (262K+), while vanilla GPT-5 degrades sharply as inputs grow. On OOLONG-Pairs -- a task requiring quadratic-complexity reasoning -- GPT-5 scores less than 0.1% F1. The RLM version scores 58%.</p>
                <p>The cost? Comparable. At the median, RLM runs are actually <em>cheaper</em> than base model calls on GPT-5, because the model selectively examines context rather than ingesting everything at once.</p>
                <p>The ecosystem is already moving. DSPy (v3.1.2+) ships with built-in RLM support. Google's Agent Development Kit has an enterprise-ready implementation with lazy file loading and parallel sub-calls. VentureBeat, InfoQ, and Towards Data Science have all published deep dives in the last month. This isn't a paper that got filed away -- it's being adopted.</p>
                <div class="callout" style="margin-top:2rem;">
                    <p>"It's a partially observable problem that you're giving the LM, where it can make logical decisions based on the structure of the task and context." -- Alex Zhang, MIT CSAIL</p>
                </div>
            </div>
        </div>
    </section>

</div>
</div>

<footer class="site-footer">
    <p>rlm.md -- Built to explain, not to impress. Content updated February 2026.</p>
</footer>

<script>
const obs = new IntersectionObserver((entries) => {
    entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); obs.unobserve(e.target); }});
}, { threshold: 0.15 });
document.querySelectorAll('.fade-in').forEach(el => obs.observe(el));
document.querySelectorAll('.nav-links a').forEach(a => {
    a.addEventListener('click', () => document.querySelector('.nav-links').classList.remove('open'));
});
</script>
</body>
</html>
