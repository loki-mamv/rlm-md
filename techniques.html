<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Techniques -- rlm.md</title>
    <meta name="description" content="Current recursive learning techniques: self-play, RLHF, constitutional AI, STaR, chain-of-thought distillation, test-time compute scaling, and more.">
    <link rel="canonical" href="https://rlm.md/techniques.html">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'%3E%3Ctext x='16' y='24' font-family='monospace' font-weight='800' font-size='14' fill='%23c9a84c' text-anchor='middle'%3Erlm%3C/text%3E%3C/svg%3E">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav class="site-nav">
    <a href="./" class="site-logo">rlm<span class="dot">.md</span></a>
    <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Menu">///</button>
    <ul class="nav-links">
        <li><a href="./">Home</a></li>
        <li><a href="fundamentals.html">Fundamentals</a></li>
        <li><a href="techniques.html" class="active">Techniques</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li><a href="resources.html">Resources</a></li>
    </ul>
</nav>

<div class="page-body">
<div class="container">

    <header class="page-header fade-in">
        <span class="hero-label">Techniques</span>
        <h1>What's actually working</h1>
        <p class="lead">A survey of recursive learning techniques that have moved from papers to production. Ordered roughly by maturity -- battle-tested first, experimental last.</p>
    </header>

    <!-- Self-Play -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Proven</span>
                <h2>Self-play</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">games</span>
                    <span class="tag">deployed</span>
                </div>
            </div>
            <div>
                <p>The OG recursive technique. Two copies of the same model play against each other. The winner's strategy gets reinforced. Over thousands of iterations, the system develops capabilities that no human demonstrated.</p>
                <p>AlphaGo Zero (Silver et al., 2017) is the canonical example. Starting from random play -- literally random -- it surpassed the strongest human Go players in 40 days. No human game data. No handcrafted features. Just the rules of Go and a lot of compute.</p>
                <p>The insight that generalized: <strong>if you have a reliable outcome signal, you don't need human demonstrations.</strong> The model discovers strategies humans never considered. Move 37 in AlphaGo's famous game against Lee Sedol was so unusual that commentators initially thought it was an error.</p>
                <pre><code><span class="comment"># Simplified self-play loop</span>
<span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="function">range</span>(<span class="number">10000</span>):
    game = <span class="function">play_game</span>(model_v_current, model_v_current)
    winner_moves = game.moves[game.winner]
    model_v_next = <span class="function">train</span>(model_v_current, winner_moves)
    <span class="keyword">if</span> <span class="function">evaluate</span>(model_v_next) > <span class="function">evaluate</span>(model_v_current):
        model_v_current = model_v_next</code></pre>
                <p>Limitations: requires a well-defined game with clear win conditions. Doesn't directly apply to open-ended generation. But the principle -- compete against yourself, keep what works -- echoes through everything that follows.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- RLHF -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Deployed at scale</span>
                <h2>RLHF</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">alignment</span>
                    <span class="tag">production</span>
                </div>
            </div>
            <div>
                <p>Reinforcement Learning from Human Feedback. The technique that made ChatGPT possible. Semi-recursive: humans provide preference data, a reward model learns from those preferences, then the language model optimizes against the reward model.</p>
                <p>The recursive part: once you have a reward model, you can generate massive amounts of synthetic preference data without additional human input. The reward model scores, the policy model improves, the reward model gets retrained on the improved outputs. Each round, the bar rises.</p>
                <p>Ouyang et al. (2022) at OpenAI showed that RLHF could take a base language model and make it dramatically more helpful, honest, and harmless -- with relatively little human feedback (tens of thousands of comparisons, not millions of examples).</p>
                <p>The catch: reward hacking. The policy model learns to exploit quirks in the reward model rather than actually getting better. This is why Anthropic moved toward constitutional AI -- a way to make the evaluation step more robust.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- Constitutional AI -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Fully recursive</span>
                <h2>Constitutional AI</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">alignment</span>
                    <span class="tag">self-evaluation</span>
                </div>
            </div>
            <div>
                <p>Anthropic's key contribution. Instead of having humans label every preference pair, give the model a set of principles (a "constitution") and let it judge its own outputs against those principles.</p>
                <p>The loop: generate a response, critique it according to the constitution, revise it, then train on the revised version. The model is simultaneously the generator, the critic, and the student.</p>
                <p>Bai et al. (2022) showed this produces models that are competitive with RLHF on helpfulness while being significantly less harmful -- and it requires far less human annotation. The constitution provides the compass; the model does the navigation.</p>
                <blockquote>The interesting thing about constitutional AI is that it's recursive all the way down. The model's ability to critique improves as its general capabilities improve. Better models are better judges of their own output.</blockquote>
                <p>This creates a virtuous cycle, but also a risk: the model's blind spots in judgment become the system's blind spots in behavior. Which is why the constitution itself is designed by humans -- it's the last piece of human input in an otherwise self-supervised loop.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- STaR -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Reasoning</span>
                <h2>STaR and self-taught reasoning</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">reasoning</span>
                    <span class="tag">bootstrapping</span>
                </div>
            </div>
            <div>
                <p>Self-Taught Reasoner (Zelikman et al., 2022). Elegant idea: ask a model to solve problems with step-by-step reasoning. Keep only the rationales that led to correct answers. Fine-tune on those. Repeat.</p>
                <p>The genius is in using the answer as a verifier for the reasoning. You don't need a separate judge -- if the final answer is right, the reasoning was probably useful. If it's wrong, discard everything.</p>
                <pre><code><span class="comment"># STaR loop</span>
<span class="keyword">for</span> round <span class="keyword">in</span> <span class="function">range</span>(N):
    rationales = model.<span class="function">generate_with_reasoning</span>(problems)
    correct = [r <span class="keyword">for</span> r <span class="keyword">in</span> rationales <span class="keyword">if</span> r.answer == r.ground_truth]
    
    <span class="comment"># Rationalization: retry failures with hints</span>
    hints = [<span class="function">give_hint</span>(r) <span class="keyword">for</span> r <span class="keyword">in</span> rationales <span class="keyword">if</span> r.answer != r.ground_truth]
    correct += [h <span class="keyword">for</span> h <span class="keyword">in</span> hints <span class="keyword">if</span> h.answer == h.ground_truth]
    
    model = <span class="function">finetune</span>(model, correct)</code></pre>
                <p>The "rationalization" step is clever -- for problems the model got wrong, provide the correct answer and ask it to generate a rationale that arrives at that answer. This bootstraps reasoning ability on harder problems.</p>
                <p>STaR demonstrated something important: <strong>you can bootstrap complex reasoning from simpler capabilities</strong>, as long as you have a way to verify correctness.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- Test-time compute -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Inference-time</span>
                <h2>Test-time compute scaling</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">reasoning</span>
                    <span class="tag">o-series</span>
                </div>
            </div>
            <div>
                <p>The big idea behind OpenAI's o1 and o3: instead of making the model bigger, let it think longer. Generate multiple reasoning chains at inference time, evaluate them, and pick the best one. More compute at test time, better answers.</p>
                <p>This is recursive learning applied at inference rather than training. The model generates candidates, scores them (via self-consistency, verifiers, or reward models), and selects. No weight updates needed -- just more thinking.</p>
                <p>Snell et al. (2024) showed that test-time compute scaling can be more efficient than training-time scaling for many tasks. A smaller model that "thinks harder" can outperform a larger model that answers immediately.</p>
                <p>The connection to recursive learning: the models used for test-time scaling were themselves trained with recursive techniques (RLHF, self-play on reasoning tasks). It's recursion at two levels -- recursive training produces a model that then reasons recursively at inference.</p>
                <div class="callout-teal callout">
                    <p>DeepSeek-R1 combined both: recursive RL training to develop reasoning ability, then test-time chain-of-thought to deploy it. The result matched OpenAI's o1 on benchmarks -- at a fraction of the training cost.</p>
                </div>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- Chain-of-thought distillation -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Compression</span>
                <h2>Chain-of-thought distillation</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">distillation</span>
                    <span class="tag">efficiency</span>
                </div>
            </div>
            <div>
                <p>A large model reasons step by step and arrives at correct answers. A smaller model learns to replicate those reasoning chains. The small model gets the reasoning ability without the cost.</p>
                <p>DeepSeek used this to create smaller distilled models from R1 -- the 7B and 14B parameter versions that retained much of the reasoning performance. The recursive element: the large model's reasoning was itself produced by recursive RL training. You're distilling the product of recursion.</p>
                <p>This creates an interesting pipeline: recursive training produces reasoning ability in a large model, distillation compresses it into a small model, and the small model can then be used as a starting point for further recursive training. Each cycle produces a more capable compact model.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <!-- Evolutionary approaches -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Experimental</span>
                <h2>Evolutionary search</h2>
                <div style="margin-top: 1rem;">
                    <span class="tag">evolution</span>
                    <span class="tag">meta-learning</span>
                </div>
            </div>
            <div>
                <p>Google DeepMind's AlphaEvolve (May 2025) uses LLMs as mutation operators in an evolutionary loop. Start with a program. Mutate it using an LLM. Evaluate the mutant. Keep improvements. Repeat.</p>
                <p>The meta-recursive twist: AlphaEvolve discovered algorithms that improve the efficiency of AI training itself. It found a better matrix multiplication kernel for TPUs -- the hardware that trains the next generation of LLMs.</p>
                <p>This is perhaps the clearest example of genuine recursive self-improvement: an AI system making the tools that make AI systems better.</p>
            </div>
        </div>
    </section>

    <!-- Next -->
    <section class="section fade-in" style="text-align: center; padding: 3rem 0;">
        <p style="color: var(--text-dim); margin: 0 auto;">Next: <a href="research.html" style="font-size: 1.1rem;">Research -- the papers that matter</a></p>
    </section>

</div>
</div>

<footer class="site-footer">
    <p>rlm.md -- Built to explain, not to impress.</p>
</footer>

<script>
const obs = new IntersectionObserver((entries) => {
    entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); obs.unobserve(e.target); }});
}, { threshold: 0.15 });
document.querySelectorAll('.fade-in').forEach(el => obs.observe(el));
document.querySelectorAll('.nav-links a').forEach(a => {
    a.addEventListener('click', () => document.querySelector('.nav-links').classList.remove('open'));
});
</script>
</body>
</html>
