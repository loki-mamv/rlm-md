<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RLM vs LLM — How Recursive Language Models Differ | rlm.md</title>
    <meta name="description" content="A deep comparison of Recursive Language Models (RLMs) and traditional Large Language Models (LLMs). Architecture, performance, cost, and when to use each.">
    <link rel="canonical" href="https://rlm.md/rlm-vs-llm.html">
    <meta property="og:title" content="RLM vs LLM — How Recursive Language Models Differ">
    <meta property="og:description" content="Architecture, performance, cost, and when to use each approach.">
    <meta property="og:url" content="https://rlm.md/rlm-vs-llm.html">
    <meta property="og:image" content="https://rlm.md/og-image.jpg">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'%3E%3Ctext x='16' y='24' font-family='monospace' font-weight='800' font-size='14' fill='%23c9a84c' text-anchor='middle'%3Erlm%3C/text%3E%3C/svg%3E">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav class="site-nav">
    <a href="index.html" class="site-logo">rlm<span class="dot">.md</span></a>
    <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Menu">///</button>
    <ul class="nav-links">
        <li><a href="./">Home</a></li>
        <li><a href="fundamentals.html">Fundamentals</a></li>
        <li><a href="techniques.html">Techniques</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="rlm-vs-llm.html" class="active">RLM vs LLM</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li><a href="resources.html">Resources</a></li>
    </ul>
</nav>

<div class="page-body">
<div class="container">

    <section class="page-header fade-in">
        <span class="split-label">Deep dive</span>
        <h1>RLM vs LLM</h1>
        <p class="lead">Both use the same underlying neural networks. The difference is how they interact with long inputs. One brute-forces it. The other thinks programmatically.</p>
    </section>

    <!-- Architecture diagrams -->
    <section class="section fade-in">
        <h2>Architecture at a glance</h2>
        <div class="arch-compare" style="margin-top: 2rem;">
            <div class="arch-col">
                <div class="arch-label">LLM (Traditional)</div>
                <div class="arch-diagram">
                    <svg viewBox="0 0 280 320" fill="none" xmlns="http://www.w3.org/2000/svg" class="arch-svg">
                        <rect x="40" y="10" width="200" height="44" rx="6" fill="#16161f" stroke="#2a2a3a" stroke-width="1"/>
                        <text x="140" y="28" text-anchor="middle" fill="#8a8698" font-size="10" font-family="Space Grotesk">272K tokens</text>
                        <text x="140" y="42" text-anchor="middle" fill="#5a5668" font-size="9" font-family="JetBrains Mono">entire input</text>
                        <line x1="140" y1="54" x2="140" y2="90" stroke="#5a5668" stroke-width="1" stroke-dasharray="4,3"/>
                        <polygon points="135,88 140,96 145,88" fill="#5a5668"/>
                        <rect x="50" y="98" width="180" height="80" rx="8" fill="#16161f" stroke="#b05a6a" stroke-width="1.5" stroke-opacity="0.6"/>
                        <text x="140" y="130" text-anchor="middle" fill="#e0dcd4" font-size="12" font-family="Space Grotesk" font-weight="600">LLM</text>
                        <text x="140" y="148" text-anchor="middle" fill="#b05a6a" font-size="9" font-family="JetBrains Mono">single forward pass</text>
                        <text x="140" y="164" text-anchor="middle" fill="#5a5668" font-size="8" font-family="JetBrains Mono">attention over ALL tokens</text>
                        <line x1="140" y1="178" x2="140" y2="214" stroke="#5a5668" stroke-width="1" stroke-dasharray="4,3"/>
                        <polygon points="135,212 140,220 145,212" fill="#5a5668"/>
                        <text x="200" y="200" fill="#b05a6a" font-size="9" font-family="JetBrains Mono" opacity="0.8">⚠ degrades</text>
                        <rect x="60" y="222" width="160" height="40" rx="6" fill="#16161f" stroke="#b05a6a" stroke-width="1" stroke-opacity="0.4"/>
                        <text x="140" y="246" text-anchor="middle" fill="#8a8698" font-size="10" font-family="Space Grotesk">Output</text>
                        <rect x="70" y="272" width="140" height="6" rx="3" fill="#1a1a24"/>
                        <rect x="70" y="272" width="50" height="6" rx="3" fill="#b05a6a" opacity="0.7"/>
                        <text x="140" y="292" text-anchor="middle" fill="#5a5668" font-size="8" font-family="JetBrains Mono">quality on long inputs</text>
                    </svg>
                </div>
            </div>
            <div class="arch-col arch-col--accent">
                <div class="arch-label">RLM (Recursive)</div>
                <div class="arch-diagram">
                    <svg viewBox="0 0 280 320" fill="none" xmlns="http://www.w3.org/2000/svg" class="arch-svg">
                        <rect x="40" y="10" width="200" height="44" rx="6" fill="#16161f" stroke="#2a2a3a" stroke-width="1"/>
                        <text x="140" y="28" text-anchor="middle" fill="#8a8698" font-size="10" font-family="Space Grotesk">10M+ tokens</text>
                        <text x="140" y="42" text-anchor="middle" fill="#5a5668" font-size="9" font-family="JetBrains Mono">stored as env variable</text>
                        <line x1="140" y1="54" x2="140" y2="70" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.6"/>
                        <line x1="140" y1="70" x2="55" y2="90" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <line x1="140" y1="70" x2="140" y2="90" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <line x1="140" y1="70" x2="225" y2="90" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <text x="140" y="82" text-anchor="middle" fill="#c9a84c" font-size="8" font-family="JetBrains Mono" opacity="0.8">decompose</text>
                        <rect x="20" y="94" width="70" height="50" rx="5" fill="#16161f" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.5"/>
                        <text x="55" y="116" text-anchor="middle" fill="#c9a84c" font-size="9" font-family="JetBrains Mono">LLM</text>
                        <text x="55" y="130" text-anchor="middle" fill="#5a5668" font-size="7" font-family="JetBrains Mono">chunk 1</text>
                        <rect x="105" y="94" width="70" height="50" rx="5" fill="#16161f" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.5"/>
                        <text x="140" y="116" text-anchor="middle" fill="#c9a84c" font-size="9" font-family="JetBrains Mono">LLM</text>
                        <text x="140" y="130" text-anchor="middle" fill="#5a5668" font-size="7" font-family="JetBrains Mono">chunk 2</text>
                        <rect x="190" y="94" width="70" height="50" rx="5" fill="#16161f" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.5"/>
                        <text x="225" y="116" text-anchor="middle" fill="#c9a84c" font-size="9" font-family="JetBrains Mono">LLM</text>
                        <text x="225" y="130" text-anchor="middle" fill="#5a5668" font-size="7" font-family="JetBrains Mono">chunk N</text>
                        <path d="M 265 110 C 275 110, 275 130, 265 130" stroke="#4a9ead" stroke-width="1" fill="none" stroke-opacity="0.5"/>
                        <polygon points="264,128 267,134 270,128" fill="#4a9ead" opacity="0.5"/>
                        <text x="268" y="155" fill="#4a9ead" font-size="7" font-family="JetBrains Mono" opacity="0.7">recurse</text>
                        <line x1="55" y1="144" x2="140" y2="174" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <line x1="140" y1="144" x2="140" y2="174" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <line x1="225" y1="144" x2="140" y2="174" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.4"/>
                        <text x="140" y="168" text-anchor="middle" fill="#c9a84c" font-size="8" font-family="JetBrains Mono" opacity="0.8">aggregate</text>
                        <rect x="60" y="178" width="160" height="44" rx="6" fill="#16161f" stroke="#4a9ead" stroke-width="1.5" stroke-opacity="0.6"/>
                        <text x="140" y="196" text-anchor="middle" fill="#e0dcd4" font-size="10" font-family="Space Grotesk" font-weight="600">REPL Environment</text>
                        <text x="140" y="212" text-anchor="middle" fill="#4a9ead" font-size="8" font-family="JetBrains Mono">merge partial results</text>
                        <line x1="140" y1="222" x2="140" y2="248" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.6"/>
                        <polygon points="135,246 140,254 145,246" fill="#c9a84c" opacity="0.6"/>
                        <rect x="60" y="256" width="160" height="40" rx="6" fill="#16161f" stroke="#c9a84c" stroke-width="1" stroke-opacity="0.6"/>
                        <text x="140" y="280" text-anchor="middle" fill="#c9a84c" font-size="10" font-family="Space Grotesk" font-weight="500">Output</text>
                        <rect x="70" y="306" width="140" height="6" rx="3" fill="#1a1a24"/>
                        <rect x="70" y="306" width="130" height="6" rx="3" fill="#c9a84c" opacity="0.7"/>
                        <text x="140" y="320" text-anchor="middle" fill="#5a5668" font-size="8" font-family="JetBrains Mono">quality on long inputs</text>
                    </svg>
                </div>
            </div>
        </div>
    </section>

    <!-- Side-by-side comparison table -->
    <section class="section fade-in">
        <h2>Head-to-head comparison</h2>
        <table>
            <thead>
                <tr>
                    <th>Dimension</th>
                    <th>LLM</th>
                    <th>RLM</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Max effective input</strong></td>
                    <td>4K–272K tokens (degrades at scale)</td>
                    <td>10M+ tokens (tested)</td>
                </tr>
                <tr>
                    <td><strong>Processing model</strong></td>
                    <td>Single forward pass — all tokens attend to all tokens</td>
                    <td>Recursive self-calls — decompose, process chunks, aggregate</td>
                </tr>
                <tr>
                    <td><strong>How input is accessed</strong></td>
                    <td>Loaded entirely into context window</td>
                    <td>Stored as environment variable, examined programmatically</td>
                </tr>
                <tr>
                    <td><strong>Scaling behavior</strong></td>
                    <td>O(n²) attention — quality drops as input grows</td>
                    <td>Recursive decomposition — quality maintained at any scale</td>
                </tr>
                <tr>
                    <td><strong>Selective attention</strong></td>
                    <td>No — must attend to everything</td>
                    <td>Yes — model decides what to examine</td>
                </tr>
                <tr>
                    <td><strong>Code execution</strong></td>
                    <td>Not part of inference</td>
                    <td>Central — model writes code in REPL to slice and process</td>
                </tr>
                <tr>
                    <td><strong>Cost at scale</strong></td>
                    <td>Linear or worse — paying for all tokens</td>
                    <td>Often cheaper — only processes relevant chunks</td>
                </tr>
                <tr>
                    <td><strong>Failure mode</strong></td>
                    <td>"Context rot" — gradually loses information</td>
                    <td>Can miss connections across chunks (mitigated by overlap strategies)</td>
                </tr>
                <tr>
                    <td><strong>Best suited for</strong></td>
                    <td>Short-to-medium inputs, conversational tasks</td>
                    <td>Book-length docs, codebases, legal corpora, deep research</td>
                </tr>
                <tr>
                    <td><strong>Training required</strong></td>
                    <td>Standard pretraining + fine-tuning</td>
                    <td>~1,000 post-training samples on any base LLM</td>
                </tr>
            </tbody>
        </table>
    </section>

    <!-- The core insight -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Core insight</span>
                <h2>Same model, different paradigm</h2>
            </div>
            <div>
                <p>An RLM is not a different model architecture. It's the <strong>same transformer</strong> — the same weights, the same attention mechanism — wrapped in a recursive execution framework.</p>
                <p>Think of it this way: an LLM is a person trying to read an entire library by cramming all the books into their field of vision at once. An RLM is the same person, but now they have a desk, a notepad, and a system. They pick up one book at a time, take notes, cross-reference, and build understanding incrementally.</p>
                <p>The key components that make this work:</p>
                <div class="callout" style="margin: 1.5rem 0;">
                    <p><strong>1. REPL Environment</strong> — The input becomes a variable in a code sandbox. The model doesn't "see" the full text. It writes Python to examine it.</p>
                </div>
                <div class="callout callout-teal" style="margin: 1.5rem 0;">
                    <p><strong>2. Recursive Self-Calls</strong> — The model can call itself on sub-problems. Process chunk 1, get a partial answer, process chunk 2 with that context, repeat.</p>
                </div>
                <div class="callout" style="margin: 1.5rem 0;">
                    <p><strong>3. Programmatic Decomposition</strong> — The model decides how to split the input. It's not fixed chunking — it's task-aware. A summarization task splits differently than a search task.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Performance section -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The numbers</span>
                <h2>Performance where it matters</h2>
            </div>
            <div>
                <p>The MIT OASYS lab tested RLM-Qwen3-8B (an 8B parameter model post-trained on just 1,000 samples) against vanilla GPT-5 on four long-context benchmarks:</p>

                <table>
                    <thead>
                        <tr><th>Benchmark</th><th>GPT-5</th><th>RLM(GPT-5)</th><th>Delta</th></tr>
                    </thead>
                    <tbody>
                        <tr><td>S-NIAH (retrieval)</td><td>High</td><td>Comparable</td><td>≈0%</td></tr>
                        <tr><td>OOLONG-Pairs (quadratic)</td><td>&lt;0.1% F1</td><td>58% F1</td><td><strong>+580x</strong></td></tr>
                        <tr><td>BrowseComp-Plus</td><td>Moderate</td><td>Strong</td><td>Significant</td></tr>
                        <tr><td>CodeQA</td><td>Degrades</td><td>Maintained</td><td>+28.3% avg</td></tr>
                    </tbody>
                </table>

                <p>The OOLONG-Pairs result is the most striking. This benchmark requires comparing information across the entire input — the kind of task where attention mechanisms fundamentally struggle at scale. GPT-5 essentially fails. The RLM version handles it because it doesn't try to attend to everything at once.</p>

                <p>On cost: at the median, RLM calls on GPT-5 are <em>cheaper</em> than vanilla GPT-5, because the model selectively examines context rather than paying for attention over all tokens.</p>
            </div>
        </div>
    </section>

    <!-- When to use each -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Practical guidance</span>
                <h2>When to use which</h2>
            </div>
            <div>
                <h3>Use a standard LLM when:</h3>
                <ul style="margin-bottom: 2rem; padding-left: 1.2rem; color: var(--text-muted);">
                    <li style="margin-bottom: 0.5rem;">Input fits comfortably in context (&lt;50K tokens)</li>
                    <li style="margin-bottom: 0.5rem;">Task is conversational or generative (not analytical)</li>
                    <li style="margin-bottom: 0.5rem;">Latency matters more than thoroughness</li>
                    <li style="margin-bottom: 0.5rem;">You need real-time streaming responses</li>
                </ul>

                <h3>Use an RLM when:</h3>
                <ul style="margin-bottom: 2rem; padding-left: 1.2rem; color: var(--text-muted);">
                    <li style="margin-bottom: 0.5rem;">Input exceeds the model's effective context window</li>
                    <li style="margin-bottom: 0.5rem;">Task requires dense reasoning over the entire input</li>
                    <li style="margin-bottom: 0.5rem;">You need to cross-reference information across documents</li>
                    <li style="margin-bottom: 0.5rem;">Accuracy matters more than speed</li>
                    <li style="margin-bottom: 0.5rem;">Processing codebases, legal documents, research corpora, or book-length content</li>
                </ul>

                <div class="callout">
                    <p>The two approaches aren't mutually exclusive. An RLM uses standard LLM calls internally — it just orchestrates them recursively. You can think of it as a meta-layer on top of any LLM.</p>
                </div>
            </div>
        </div>
    </section>

    <!-- Ecosystem -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Adoption</span>
                <h2>The ecosystem is moving</h2>
            </div>
            <div>
                <p><strong>DSPy v3.1.2+</strong> ships with built-in RLM support. If you're already using DSPy for prompt programming, adding recursive processing is a configuration change.</p>
                <p><strong>Google's Agent Development Kit (ADK)</strong> has an enterprise-ready implementation with lazy file loading and parallel sub-calls — optimized for production workloads.</p>
                <p>The original <strong>MIT paper</strong> (<a href="https://arxiv.org/abs/2512.24601" target="_blank">arXiv:2512.24601</a>) includes the full algorithm, training data, and post-training recipe. The model weights for RLM-Qwen3-8B are available on HuggingFace. The <code>pip install rlm</code> package provides a reference implementation.</p>
                <p>This isn't a research curiosity anymore. It's being deployed in production systems for document processing, code analysis, and deep research applications.</p>
            </div>
        </div>
    </section>

    <hr class="divider">

    <section class="section fade-in" style="text-align: center; padding: 2rem 0;">
        <p style="color: var(--text-muted); margin: 0 auto;">Ready to go deeper? Start with <a href="fundamentals.html">how RLMs work</a>, or see <a href="research.html">the benchmark results</a>.</p>
    </section>

</div>
</div>

<footer class="site-footer">
    <p>rlm.md — Built to explain, not to impress. Content updated February 2026.</p>
</footer>

<script>
const obs = new IntersectionObserver((entries) => {
    entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); obs.unobserve(e.target); }});
}, { threshold: 0.15 });
document.querySelectorAll('.fade-in').forEach(el => obs.observe(el));
document.querySelectorAll('.nav-links a').forEach(a => {
    a.addEventListener('click', () => document.querySelector('.nav-links').classList.remove('open'));
});
</script>
</body>
</html>
