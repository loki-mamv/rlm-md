<!DOCTYPE html>
<html lang="en">
<head>
<!-- Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6H85DH0R8S"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-6H85DH0R8S');
</script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fundamentals -- How Recursive Language Models Work | rlm.md</title>
    <meta name="description" content="The technical foundations of Recursive Language Models: the decompose-recurse-aggregate pattern, REPL environments, symbolic recursion, and why treating prompts as external variables changes everything.">
    <meta name="keywords" content="recursive language models, RLM fundamentals, REPL environment, symbolic recursion, decompose recurse aggregate, long context processing">
    <link rel="canonical" href="https://rlm.md/fundamentals.html">
    <meta property="og:title" content="Fundamentals -- How Recursive Language Models Work">
    <meta property="og:description" content="The decompose-recurse-aggregate pattern that lets LLMs handle 10M+ token inputs.">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'%3E%3Ctext x='16' y='24' font-family='monospace' font-weight='800' font-size='14' fill='%23c9a84c' text-anchor='middle'%3Erlm%3C/text%3E%3C/svg%3E">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav class="site-nav">
    <a href="index.html" class="site-logo">rlm<span class="dot">.md</span></a>
    <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Menu">///</button>
    <ul class="nav-links">
        <li><a href="./">Home</a></li>
        <li><a href="fundamentals.html" class="active">Fundamentals</a></li>
        <li><a href="techniques.html">Techniques</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="rlm-vs-llm.html">RLM vs LLM</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li><a href="resources.html">Resources</a></li>
        <li><a href="blog/">Blog</a></li>
    </ul>
</nav>

<div class="page-body">
<div class="container">

    <section class="hero fade-in">
        <span class="hero-label">Fundamentals</span>
        <h1>The prompt is not <span class="accent">an input</span>. It's an <span class="accent">environment</span>.</h1>
        <p class="hero-sub">That single design decision is what separates RLMs from every other long-context approach. Instead of feeding a million tokens into a Transformer and hoping for the best, you load the prompt into a REPL as a variable and let the model write code to interact with it.</p>
    </section>

    <hr class="divider">

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The core insight</span>
                <h2>Why shoving everything into context doesn't work</h2>
            </div>
            <div>
                <p>Every LLM has a context window -- a maximum number of tokens it can process at once. The industry keeps making these bigger: 128K, 272K, 1M. But size isn't the issue. <strong>Quality</strong> is.</p>
                <p>Even within their stated limits, models exhibit <em>context rot</em>: performance degrades as prompts get longer, especially on tasks that require reasoning over the entire input rather than just locating a specific fact. GPT-5 handles needle-in-a-haystack fine at 200K tokens. Ask it to aggregate information from every paragraph in a 200K-token document, and it falls apart.</p>
                <p>The degradation gets worse as task complexity increases. Constant-complexity tasks (find one thing) survive longer contexts. Linear-complexity tasks (process every chunk) degrade faster. Quadratic-complexity tasks (reason about pairs of chunks) collapse almost immediately.</p>
                <p>RLMs sidestep this entirely. The neural network never sees the full prompt. It only sees metadata about it -- length, a short prefix, type information -- and writes code to interact with it piece by piece.</p>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The architecture</span>
                <h2>How an RLM actually works</h2>
            </div>
            <div>
                <p>An RLM wraps any base language model with an inference-time scaffold. The flow:</p>
                <ol>
                    <li><strong>Initialize a REPL.</strong> Given an arbitrary-length prompt P, the RLM starts a persistent programming environment (Python REPL). P is stored as a string variable inside this environment. The model also gets a function for invoking sub-RLM calls.</li>
                    <li><strong>Provide metadata, not content.</strong> The root model receives only constant-size metadata about P: its length, a short prefix, how to access slices of it. The full text of P never enters the model's context window.</li>
                    <li><strong>Model writes code.</strong> The model generates code that peeks into P, slices it, transforms it, and launches sub-RLM calls on the slices. These sub-calls are themselves full RLMs that can recurse further.</li>
                    <li><strong>Execute and observe.</strong> The REPL runs the code, updates state, and returns only metadata about stdout back to the model. Intermediate results live as variables in the REPL, not in the model's context.</li>
                    <li><strong>Aggregate and return.</strong> When the model sets a special "Final" variable in the REPL, iteration stops and that value becomes the response.</li>
                </ol>
                <p>The key: at every level of recursion, the model's context window only contains constant-size turns. All the heavy data lives in REPL variables. This is what makes unbounded input processing possible.</p>
            </div>
        </div>
    </section>

    <!-- Visual diagram using CSS -->
    <section class="section fade-in">
        <span class="split-label">Visualized</span>
        <h2>The recursive call pattern</h2>
        <div style="margin-top: 2rem; padding: 2rem; border: 1px solid var(--border); border-radius: 8px; background: var(--bg-alt, rgba(255,255,255,0.02)); font-family: monospace; font-size: 0.85rem; line-height: 1.8; overflow-x: auto;">
<pre style="margin:0; color: var(--text);">
User Prompt P (e.g., 10M tokens)
    |
    v
[RLM Root] -- sees: len(P)=10M, P[:200]="The first..."
    |
    |-- writes: chunks = [P[i:i+8000] for i in range(0, len(P), 8000)]
    |-- writes: results = [sub_rlm(f"Summarize: {c}") for c in chunks]
    |                         |
    |                         +--[Sub-RLM 1] processes chunk 1 (8K tokens)
    |                         +--[Sub-RLM 2] processes chunk 2 (8K tokens)
    |                         +--[Sub-RLM 3] processes chunk 3 (8K tokens)
    |                         +-- ... (1,250 sub-calls for 10M tokens)
    |
    |-- writes: combined = "\n".join(results)
    |-- writes: Final = sub_rlm(f"Given these summaries: {combined}, answer: ...")
    |
    v
Response Y
</pre>
        </div>
        <p style="margin-top: 1rem; color: var(--text-dim); font-size: 0.85rem;">Each sub-RLM is itself a full RLM that can recurse further if its input is still too large. The recursion bottoms out when chunks fit comfortably in the base model's context window.</p>
    </section>

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Three design choices</span>
                <h2>What makes RLMs different from "just using agents"</h2>
            </div>
            <div>
                <p>The paper identifies three specific design decisions that separate RLMs from existing agent scaffolds:</p>
                <p><strong>1. The prompt is a variable, not context.</strong> Coding agents and retrieval agents put the user prompt directly into the LLM's context window. An RLM stores it externally. This sounds trivial but it's the entire game -- it means the model is never bounded by its context window with respect to user input.</p>
                <p><strong>2. Output is symbolic, not autoregressive.</strong> Standard scaffolds ask the model to generate its final answer token-by-token into the context window, which means outputs are also bounded by the window. RLMs build up the response in REPL variables, enabling unbounded output length.</p>
                <p><strong>3. Recursion is programmatic, not verbal.</strong> Previous self-delegation approaches (like Anthropic's sub-agent patterns) let models invoke themselves, but the sub-calls are generated autoregressively -- one at a time, limited by output length. RLMs write <em>programs</em> that launch sub-calls inside loops, enabling the model to invoke itself O(|P|) or even O(|P|^2) times through a few lines of code.</p>
                <p>Point 3 is the killer. A standard agent might verbalize "now process chunk 1... now process chunk 2..." and run out of context after a dozen chunks. An RLM writes <code>for chunk in chunks: results.append(sub_rlm(chunk))</code> and processes thousands.</p>
            </div>
        </div>
    </section>

    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">Complexity classes</span>
                <h2>Constant, linear, quadratic -- and why it matters</h2>
            </div>
            <div>
                <p>Not all long-context tasks are created equal. The paper categorizes them by how processing complexity scales with input length:</p>
                <p><strong>Constant complexity</strong> -- tasks like needle-in-a-haystack where you're looking for one thing regardless of input size. Frontier models handle these reasonably well even at long contexts. RLMs help but the gap is smaller.</p>
                <p><strong>Linear complexity</strong> -- tasks like OOLONG where the answer depends on processing every chunk of the input. These break standard models quickly. RLMs with GPT-5 outperform vanilla GPT-5 by 28.4% here.</p>
                <p><strong>Quadratic complexity</strong> -- tasks like OOLONG-Pairs where you need to reason about <em>pairs</em> of chunks. Vanilla GPT-5 scores below 0.1% F1. RLM(GPT-5) scores 58% F1. The gap is comical.</p>
                <p>This hierarchy is the real insight. Context windows aren't just too small -- they're the wrong abstraction for information-dense tasks. No amount of window expansion will help a model that needs to do O(n^2) semantic work in a single forward pass.</p>
            </div>
        </div>
    </section>

</div>
</div>

<footer class="site-footer">
    <p>rlm.md -- Built to explain, not to impress. Content updated February 2026.</p>
</footer>

<script>
const obs = new IntersectionObserver((entries) => {
    entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); obs.unobserve(e.target); }});
}, { threshold: 0.01 });
document.querySelectorAll('.fade-in').forEach(el => obs.observe(el));
document.querySelectorAll('.nav-links a').forEach(a => {
    a.addEventListener('click', () => document.querySelector('.nav-links').classList.remove('open'));
});
</script>
</body>
</html>
