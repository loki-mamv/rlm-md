<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research -- rlm.md</title>
    <meta name="description" content="Key papers in recursive learning: from AlphaGo Zero to DeepSeek-R1. Annotated, opinionated, and chronologically ordered.">
    <link rel="canonical" href="https://rlm.md/research.html">
    <link rel="icon" type="image/svg+xml" href="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 32 32'%3E%3Ctext x='16' y='24' font-family='monospace' font-weight='800' font-size='14' fill='%23c9a84c' text-anchor='middle'%3Erlm%3C/text%3E%3C/svg%3E">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<nav class="site-nav">
    <a href="./" class="site-logo">rlm<span class="dot">.md</span></a>
    <button class="nav-toggle" onclick="document.querySelector('.nav-links').classList.toggle('open')" aria-label="Menu">///</button>
    <ul class="nav-links">
        <li><a href="./">Home</a></li>
        <li><a href="fundamentals.html">Fundamentals</a></li>
        <li><a href="techniques.html">Techniques</a></li>
        <li><a href="research.html" class="active">Research</a></li>
        <li><a href="applications.html">Applications</a></li>
        <li><a href="resources.html">Resources</a></li>
    </ul>
</nav>

<div class="page-body">
<div class="container">

    <header class="page-header fade-in">
        <span class="hero-label">Research</span>
        <h1>The papers that matter</h1>
        <p class="lead">A chronological tour of the key results in recursive learning. Not comprehensive -- opinionated. These are the papers that changed what people thought was possible.</p>
    </header>

    <!-- Timeline -->
    <section class="section fade-in">

        <div class="timeline-item">
            <div class="timeline-year">2017</div>
            <div>
                <h3>Mastering the Game of Go without Human Knowledge</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Silver et al. -- DeepMind / Nature</p>
                <p>AlphaGo Zero. The paper that proved recursive self-play could surpass all human expertise from a blank slate. 40 days of self-play, zero human game data, superhuman performance. This isn't just a Go result -- it's an existence proof that recursive learning works at scale.</p>
                <p><a href="https://www.nature.com/articles/nature24270">nature.com/articles/nature24270</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2017</div>
            <div>
                <h3>Mastering Chess and Shogi by Self-Play</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Silver et al. -- DeepMind</p>
                <p>AlphaZero. Generalized the approach to chess and shogi. Same algorithm, different games, same result: superhuman performance through pure self-play. Demonstrated that the recursive approach wasn't specific to Go but a general principle.</p>
                <p><a href="https://arxiv.org/abs/1712.01815">arxiv.org/abs/1712.01815</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2020</div>
            <div>
                <h3>Learning to Summarize from Human Feedback</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Stiennon et al. -- OpenAI</p>
                <p>Early RLHF applied to summarization. Showed that training a reward model on human preferences, then optimizing a policy against it, produced summaries that humans preferred over the base model's output. The template for everything that followed.</p>
                <p><a href="https://arxiv.org/abs/2009.01325">arxiv.org/abs/2009.01325</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2022</div>
            <div>
                <h3>Training Language Models to Follow Instructions with Human Feedback</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Ouyang et al. -- OpenAI</p>
                <p>InstructGPT. The paper behind ChatGPT's instruction-following ability. RLHF at scale. Showed that a relatively small amount of human preference data, combined with recursive reward model training, could transform a base language model into something genuinely useful.</p>
                <p><a href="https://arxiv.org/abs/2203.02155">arxiv.org/abs/2203.02155</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2022</div>
            <div>
                <h3>STaR: Bootstrapping Reasoning With Reasoning</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Zelikman et al. -- Stanford</p>
                <p>Self-Taught Reasoner. Generate rationales, keep the ones that lead to correct answers, train on them, repeat. Simple, effective, and the intellectual ancestor of much of what o1 and R1 do. The rationalization trick -- giving the model the answer and asking it to explain why -- was particularly clever.</p>
                <p><a href="https://arxiv.org/abs/2203.14465">arxiv.org/abs/2203.14465</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2022</div>
            <div>
                <h3>Constitutional AI: Harmlessness from AI Feedback</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Bai et al. -- Anthropic</p>
                <p>The model critiques and revises its own outputs according to a set of principles. Then trains on the revisions. Removed most of the human labeling from the alignment loop. A fully recursive alignment technique that gets better as the model gets smarter.</p>
                <p><a href="https://arxiv.org/abs/2212.08073">arxiv.org/abs/2212.08073</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2023</div>
            <div>
                <h3>The Curse of Recursion: Training on Generated Data Makes Models Forget</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Shumailov et al. -- Oxford / Cambridge</p>
                <p>The cautionary tale. Showed that naively training on model-generated data causes model collapse -- progressive loss of diversity and accuracy. Essential reading for anyone building recursive systems. Not a reason to avoid recursion, but a reason to do it carefully.</p>
                <p><a href="https://arxiv.org/abs/2305.17493">arxiv.org/abs/2305.17493</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2024</div>
            <div>
                <h3>Scaling LLM Test-Time Compute Optimally</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Snell et al. -- UC Berkeley</p>
                <p>Showed that spending more compute at inference (generating and evaluating multiple solutions) can be more cost-effective than training a larger model. Formalized the test-time compute scaling laws. The theoretical backbone of the o-series models.</p>
                <p><a href="https://arxiv.org/abs/2408.03314">arxiv.org/abs/2408.03314</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2024</div>
            <div>
                <h3>AlphaProof and AlphaGeometry 2</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Google DeepMind</p>
                <p>AlphaProof solved 4 of 6 problems at the International Mathematical Olympiad -- silver medal level -- using a recursive approach: generate proof candidates, verify with Lean (a formal proof assistant), train on successful proofs. Applied the AlphaZero philosophy to mathematical reasoning.</p>
                <p><a href="https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/">deepmind.google blog post</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2025</div>
            <div>
                <h3>DeepSeek-R1: Incentivizing Reasoning via Reinforcement Learning</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">DeepSeek AI -- Published in Nature (Sept 2025)</p>
                <p>The paper that shook the industry. Pure RL, no supervised fine-tuning on reasoning demonstrations. R1-Zero developed chain-of-thought reasoning spontaneously through reward signals alone. Then R1 added a small amount of cold-start data and scaled further. Matched o1 on benchmarks. Open-sourced the weights. Published in Nature.</p>
                <p>The key finding: <strong>reasoning emerges from pure recursive reinforcement learning</strong>. You don't need to teach the model how to reason. You need to reward it for getting the right answer and let recursion do the rest.</p>
                <p><a href="https://arxiv.org/abs/2501.12948">arxiv.org/abs/2501.12948</a></p>
            </div>
        </div>

        <div class="timeline-item">
            <div class="timeline-year">2025</div>
            <div>
                <h3>AlphaEvolve</h3>
                <p style="color: var(--text-dim); font-size: 0.85rem; margin-bottom: 0.75rem;">Google DeepMind -- May 2025</p>
                <p>An evolutionary coding agent that uses LLMs to generate and mutate programs, evaluates them against fitness functions, and selects the best. Found improvements to core algorithms used in Google's infrastructure -- including matrix multiplication kernels for TPUs. AI improving the hardware that trains AI.</p>
                <p><a href="https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/">deepmind.google blog post</a></p>
            </div>
        </div>

    </section>

    <hr class="divider">

    <!-- Pattern -->
    <section class="section fade-in">
        <div class="split">
            <div>
                <span class="split-label">The pattern</span>
                <h2>What ties these together</h2>
            </div>
            <div>
                <p>Read these papers chronologically and a clear trajectory emerges:</p>
                <p><strong>2017:</strong> Recursive learning works in games with perfect information.</p>
                <p><strong>2020-2022:</strong> It works for aligning language models, if humans stay in the loop.</p>
                <p><strong>2022-2023:</strong> The human can be partially replaced by the model itself (constitutional AI, STaR).</p>
                <p><strong>2024:</strong> Recursive reasoning produces olympiad-level mathematics.</p>
                <p><strong>2025:</strong> Pure RL produces emergent reasoning. AI systems improve AI infrastructure.</p>
                <p>Each year, the human's role in the loop gets smaller. The question driving current research: how small can it get?</p>
            </div>
        </div>
    </section>

    <section class="section fade-in" style="text-align: center; padding: 3rem 0;">
        <p style="color: var(--text-dim); margin: 0 auto;">Next: <a href="applications.html" style="font-size: 1.1rem;">Applications -- where it's deployed</a></p>
    </section>

</div>
</div>

<footer class="site-footer">
    <p>rlm.md -- Built to explain, not to impress.</p>
</footer>

<script>
const obs = new IntersectionObserver((entries) => {
    entries.forEach(e => { if (e.isIntersecting) { e.target.classList.add('visible'); obs.unobserve(e.target); }});
}, { threshold: 0.15 });
document.querySelectorAll('.fade-in').forEach(el => obs.observe(el));
document.querySelectorAll('.nav-links a').forEach(a => {
    a.addEventListener('click', () => document.querySelector('.nav-links').classList.remove('open'));
});
</script>
</body>
</html>
